{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from transformers import BartForSequenceClassification\n",
    "from transformers import BartTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-mnli\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "model = BartForSequenceClassification.from_pretrained(\"facebook/bart-large-mnli\")\n",
    "model.config.dropout = 0.5\n",
    "model.classification_head.dense = torch.nn.Linear(in_features=model.config.d_model, out_features=model.config.d_model, bias=True)\n",
    "model.classification_head.out_proj = torch.nn.Linear(in_features=model.config.d_model, out_features=5, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_dataset = pd.DataFrame(pd.read_csv('/Users/aryan/Actual-Coding/CDAC/us-patent-phrase-to-phrase-matching/train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mapping = {\n",
    "    0: 0,\n",
    "    0.25: 1,\n",
    "    0.5: 2,\n",
    "    0.75: 3,\n",
    "    1: 4\n",
    "}\n",
    "\n",
    "train_dataset['score'] = train_dataset['score'].map(score_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_dataset, cross_verify_data = train_test_split(train_dataset, test_size=0.1)  # 10% for cross-verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from random import randint\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def replace_synonym(sentence, num_replacements=1):\n",
    "    words = sentence.split()\n",
    "    new_sentence = sentence\n",
    "    for _ in range(num_replacements):\n",
    "        word_to_replace = words[randint(0, len(words)-1)]\n",
    "        synonyms = [syn.name().split('.')[0] for syn in wordnet.synsets(word_to_replace) if syn.name().split('.')[0] != word_to_replace]\n",
    "        if synonyms:\n",
    "            new_sentence = new_sentence.replace(word_to_replace, synonyms[0], 1)\n",
    "    return new_sentence\n",
    "\n",
    "# Augmenting data\n",
    "df_train_dataset['target'] = df_train_dataset['target'].apply(lambda x: replace_synonym(x, num_replacements=5))\n",
    "df_train_dataset['anchor'] = df_train_dataset['anchor'].apply(lambda x: replace_synonym(x, num_replacements=5))\n",
    "cross_verify_data['target'] = cross_verify_data['target'].apply(lambda x: replace_synonym(x, num_replacements=5))\n",
    "cross_verify_data['anchor'] = cross_verify_data['anchor'].apply(lambda x: replace_synonym(x, num_replacements=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_actual = df_train_dataset['score']\n",
    "score_list = score_actual[1:51].tolist()\n",
    "score_train = pd.DataFrame(score_actual[1:51])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score\n",
       "2        21\n",
       "1        17\n",
       "0         8\n",
       "3         3\n",
       "4         1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15255</th>\n",
       "      <td>3792820488d01427</td>\n",
       "      <td>learn care parameter</td>\n",
       "      <td>learn music</td>\n",
       "      <td>H04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17272</th>\n",
       "      <td>aebda813e7f88214</td>\n",
       "      <td>complect lever</td>\n",
       "      <td>employment aircraft_aircraft_aircraft_carrier</td>\n",
       "      <td>F16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30425</th>\n",
       "      <td>e01731d390bb9846</td>\n",
       "      <td>transformation connection</td>\n",
       "      <td>temperature transformation</td>\n",
       "      <td>A47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11162</th>\n",
       "      <td>781b2819ec953e95</td>\n",
       "      <td>acme position</td>\n",
       "      <td>acme design acme</td>\n",
       "      <td>B66</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15580</th>\n",
       "      <td>007d8b9272c77ad3</td>\n",
       "      <td>senior_senior_high_school_school gradient char...</td>\n",
       "      <td>toe centrifuge</td>\n",
       "      <td>B03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>7c0ead66438745af</td>\n",
       "      <td>acerb assimilation</td>\n",
       "      <td>acerb</td>\n",
       "      <td>B01</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13579</th>\n",
       "      <td>95adeb55a1ca9876</td>\n",
       "      <td>magnetic_field governor</td>\n",
       "      <td>aide control_condition_condition cringle</td>\n",
       "      <td>H01</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16909</th>\n",
       "      <td>d558f6327da12c15</td>\n",
       "      <td>inorganic loanblend</td>\n",
       "      <td>luminosity emit</td>\n",
       "      <td>B32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29355</th>\n",
       "      <td>26c3c6dc6174b589</td>\n",
       "      <td>sealing_waxing_waxing_wax dentition</td>\n",
       "      <td>dentition whiten</td>\n",
       "      <td>F01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20647</th>\n",
       "      <td>36b3f277c9861282</td>\n",
       "      <td>motion to scope</td>\n",
       "      <td>pry</td>\n",
       "      <td>F15</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                             anchor  \\\n",
       "15255  3792820488d01427                               learn care parameter   \n",
       "17272  aebda813e7f88214                                     complect lever   \n",
       "30425  e01731d390bb9846                          transformation connection   \n",
       "11162  781b2819ec953e95                                      acme position   \n",
       "15580  007d8b9272c77ad3  senior_senior_high_school_school gradient char...   \n",
       "268    7c0ead66438745af                                 acerb assimilation   \n",
       "13579  95adeb55a1ca9876                            magnetic_field governor   \n",
       "16909  d558f6327da12c15                                inorganic loanblend   \n",
       "29355  26c3c6dc6174b589                sealing_waxing_waxing_wax dentition   \n",
       "20647  36b3f277c9861282                                    motion to scope   \n",
       "\n",
       "                                              target context  score  \n",
       "15255                                    learn music     H04      0  \n",
       "17272  employment aircraft_aircraft_aircraft_carrier     F16      1  \n",
       "30425                     temperature transformation     A47      0  \n",
       "11162                               acme design acme     B66      2  \n",
       "15580                                 toe centrifuge     B03      0  \n",
       "268                                            acerb     B01      2  \n",
       "13579       aide control_condition_condition cringle     H01      2  \n",
       "16909                                luminosity emit     B32      1  \n",
       "29355                               dentition whiten     F01      0  \n",
       "20647                                            pry     F15      2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_dataset['hypothesis'] = df_train_dataset['anchor']\n",
    "inputs = df_train_dataset['hypothesis']\n",
    "hypothesis_list = inputs[1:51].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "premise = df_train_dataset['target']\n",
    "premise_list = premise[1:51].tolist()\n",
    "premise_for_testing = pd.DataFrame(premise_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "input_ids = tokenizer(premise_list, hypothesis_list, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "attention_masks = input_ids[\"attention_mask\"]\n",
    "labels = torch.tensor(score_list, dtype=torch.long)\n",
    "labels_one_hot = torch.nn.functional.one_hot(labels, num_classes=5)\n",
    "labels_one_hot = labels_one_hot.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Up Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encoded_texts, labels):\n",
    "        self.encoded_texts = encoded_texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encoded_texts.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "train_dataset = CustomDataset(input_ids, labels_one_hot)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a56ca154e78442a3b3cd58357250d686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 26.1537, 'train_samples_per_second': 1.912, 'train_steps_per_second': 0.382, 'train_loss': 0.9305903434753418, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=0.9305903434753418, metrics={'train_runtime': 26.1537, 'train_samples_per_second': 1.912, 'train_steps_per_second': 0.382, 'train_loss': 0.9305903434753418, 'epoch': 1.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./patents-output\",\n",
    "    per_device_train_batch_size = 5,\n",
    "    num_train_epochs = 1,\n",
    "    learning_rate = 1e-2,\n",
    "    save_steps = 500,\n",
    "    save_total_limit = 2,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps = 500,\n",
    "    max_grad_norm=1.0,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model, # type: ignore\n",
    "    args = training_args,\n",
    "    data_collator = data_collator,\n",
    "    train_dataset = train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_verify_hypothesis = cross_verify_data['anchor'].tolist()\n",
    "cross_verify_premise = cross_verify_data['target'].tolist()\n",
    "cross_verify_score = cross_verify_data['score'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_verify_input_ids = tokenizer(cross_verify_premise, cross_verify_hypothesis, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "cross_verify_score_tensor = torch.tensor(cross_verify_score, dtype=torch.long)\n",
    "cross_verify_labels_one_hot = torch.nn.functional.one_hot(cross_verify_score_tensor, num_classes=5).float()\n",
    "cross_verify_dataset = CustomDataset(cross_verify_input_ids, cross_verify_labels_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a10ebee02741ee991cce82c7927e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "cross_verify_results = trainer.predict(cross_verify_dataset)\n",
    "cross_verify_predictions = cross_verify_results.predictions\n",
    "cross_verify_label_ids = cross_verify_results.label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       755\n",
      "           1       1.00      1.00      1.00      1127\n",
      "           2       1.00      1.00      1.00      1273\n",
      "           3       1.00      1.00      1.00       382\n",
      "           4       1.00      1.00      1.00       111\n",
      "\n",
      "   micro avg       1.00      1.00      1.00      3648\n",
      "   macro avg       1.00      1.00      1.00      3648\n",
      "weighted avg       1.00      1.00      1.00      3648\n",
      " samples avg       1.00      1.00      1.00      3648\n",
      "\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(classification_report(cross_verify_labels_one_hot, cross_verify_label_ids)) # type: ignore\n",
    "print(\"Accuracy:\", accuracy_score(cross_verify_labels_one_hot, cross_verify_label_ids)) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: id\n",
      "True Label: tensor([0., 0., 1., 0., 0.])\n",
      "Predicted Label: [0. 0. 1. 0. 0.]\n",
      "--------------------------------------------------\n",
      "Input: anchor\n",
      "True Label: tensor([0., 1., 0., 0., 0.])\n",
      "Predicted Label: [0. 1. 0. 0. 0.]\n",
      "--------------------------------------------------\n",
      "Input: target\n",
      "True Label: tensor([0., 1., 0., 0., 0.])\n",
      "Predicted Label: [0. 1. 0. 0. 0.]\n",
      "--------------------------------------------------\n",
      "Input: context\n",
      "True Label: tensor([0., 1., 0., 0., 0.])\n",
      "Predicted Label: [0. 1. 0. 0. 0.]\n",
      "--------------------------------------------------\n",
      "Input: score\n",
      "True Label: tensor([0., 0., 0., 0., 1.])\n",
      "Predicted Label: [0. 0. 0. 0. 1.]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for input, true_label, predicted_label in zip(cross_verify_data[:10], cross_verify_labels_one_hot[:10], cross_verify_label_ids[:10]): # type: ignore\n",
    "    print(f\"Input: {input}\")\n",
    "    print(f\"True Label: {true_label}\")\n",
    "    print(f\"Predicted Label: {predicted_label}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 205885440 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\aryan\\Actual-Coding\\CDAC\\zeroshot_patents_bart.ipynb Cell 24\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/aryan/Actual-Coding/CDAC/zeroshot_patents_bart.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m output_dir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./patents-output/bart\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/aryan/Actual-Coding/CDAC/zeroshot_patents_bart.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39;49msave_pretrained(output_dir)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/aryan/Actual-Coding/CDAC/zeroshot_patents_bart.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m tokenizer\u001b[39m.\u001b[39msave_pretrained(output_dir)\n",
      "File \u001b[1;32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\modeling_utils.py:1847\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[1;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, **kwargs)\u001b[0m\n\u001b[0;32m   1845\u001b[0m         safe_save_file(shard, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(save_directory, shard_file), metadata\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mformat\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m})\n\u001b[0;32m   1846\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1847\u001b[0m         save_function(shard, os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(save_directory, shard_file))\n\u001b[0;32m   1849\u001b[0m \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1850\u001b[0m     path_to_weights \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(save_directory, _add_variant(WEIGHTS_NAME, variant))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\serialization.py:505\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m    504\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m--> 505\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[0;32m    506\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    507\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\serialization.py:736\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[39m# given that we copy things around anyway, we might use storage.cpu()\u001b[39;00m\n\u001b[0;32m    733\u001b[0m \u001b[39m# this means to that to get tensors serialized, you need to implement\u001b[39;00m\n\u001b[0;32m    734\u001b[0m \u001b[39m# .cpu() on the underlying Storage\u001b[39;00m\n\u001b[0;32m    735\u001b[0m \u001b[39mif\u001b[39;00m storage\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 736\u001b[0m     storage \u001b[39m=\u001b[39m storage\u001b[39m.\u001b[39;49mcpu()\n\u001b[0;32m    737\u001b[0m \u001b[39m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[0;32m    738\u001b[0m num_bytes \u001b[39m=\u001b[39m storage\u001b[39m.\u001b[39mnbytes()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\storage.py:131\u001b[0m, in \u001b[0;36m_StorageBase.cpu\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39m\"\"\"Returns a CPU copy of this storage if it's not already on the CPU\"\"\"\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 131\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mUntypedStorage(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize())\u001b[39m.\u001b[39mcopy_(\u001b[39mself\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 205885440 bytes."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "output_dir = \"./patents-output/bart\"\n",
    "\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3378, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\aryan\\AppData\\Local\\Temp\\ipykernel_12212\\4188270300.py\", line 5, in <module>\n",
      "    tf_model = TFAutoModelForSequenceClassification.from_pretrained(output_dir, from_pt=True, config=config)\n",
      "  File \"c:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 493, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "  File \"c:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2880, in from_pretrained\n",
      "    return load_pytorch_checkpoint_in_tf2_model(\n",
      "  File \"c:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\modeling_tf_pytorch_utils.py\", line 185, in load_pytorch_checkpoint_in_tf2_model\n",
      "    pt_state_dict.update(torch.load(pt_path, map_location=\"cpu\"))\n",
      "  File \"C:\\Users\\aryan\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\serialization.py\", line 900, in load\n",
      "    return _load(opened_zipfile,\n",
      "  File \"C:\\Users\\aryan\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\serialization.py\", line 1287, in _load\n",
      "    result = unpickler.load()\n",
      "  File \"C:\\Users\\aryan\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\serialization.py\", line 1257, in persistent_load\n",
      "    typed_storage = load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n",
      "  File \"C:\\Users\\aryan\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\serialization.py\", line 1222, in load_tensor\n",
      "    storage = zip_file.get_storage_from_record(name, numel, torch.UntypedStorage)._typed_storage()._untyped_storage\n",
      "RuntimeError: [enforce fail at alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 16777216 bytes.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 1997, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"c:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1112, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"c:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1006, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"c:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 859, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"c:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 793, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"c:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 848, in get_records\n",
      "    return list(stack_data.FrameInfo.stack_data(etb, options=options))[tb_offset:]\n",
      "  File \"c:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\stack_data\\core.py\", line 597, in stack_data\n",
      "    yield from collapse_repeated(\n",
      "  File \"c:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\stack_data\\utils.py\", line 84, in collapse_repeated\n",
      "    yield from map(mapper, original_group)\n",
      "  File \"c:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\stack_data\\core.py\", line 587, in mapper\n",
      "    return cls(f, options)\n",
      "  File \"c:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\stack_data\\core.py\", line 551, in __init__\n",
      "    self.executing = Source.executing(frame_or_tb)\n",
      "  File \"c:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\executing\\executing.py\", line 323, in executing\n",
      "    source = cls.for_frame(frame)\n",
      "  File \"c:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\executing\\executing.py\", line 247, in for_frame\n",
      "    return cls.for_filename(frame.f_code.co_filename, frame.f_globals or {}, use_cache)\n",
      "  File \"c:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\executing\\executing.py\", line 275, in for_filename\n",
      "    return cls._for_filename_and_lines(filename, lines)\n",
      "  File \"c:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\executing\\executing.py\", line 285, in _for_filename_and_lines\n",
      "    result = source_cache[(filename, lines)] = cls(filename, lines)\n",
      "  File \"c:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\stack_data\\core.py\", line 95, in __init__\n",
      "    super(Source, self).__init__(*args, **kwargs)\n",
      "  File \"c:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\executing\\executing.py\", line 228, in __init__\n",
      "    self.tree = ast.parse(ast_text, filename=filename)\n",
      "  File \"c:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\ast.py\", line 50, in parse\n",
      "    return compile(source, filename, mode, flags,\n",
      "MemoryError\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(output_dir)\n",
    "tf_model = TFAutoModelForSequenceClassification.from_pretrained(output_dir, from_pt=True, config=config)\n",
    "tf_model.save_pretrained(output_dir, saved_model=True)\n",
    "tf_model.save_weights(output_dir + '/tf_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_model = model.from_pretrained('./patents-output/deberta')\n",
    "#loaded_tokenizer = tokenizer.from_pretrained('./patents-output/deberta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BartForSequenceClassification' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\aryan\\Actual-Coding\\CDAC\\zeroshot_patents_bart.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/aryan/Actual-Coding/CDAC/zeroshot_patents_bart.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49msave(output_dir)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/aryan/Actual-Coding/CDAC/zeroshot_patents_bart.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/aryan/Actual-Coding/CDAC/zeroshot_patents_bart.ipynb#X34sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m tokenizer_json \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mto_json()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1630\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1628\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1629\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1630\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1631\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BartForSequenceClassification' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "model.save(output_dir)\n",
    "\n",
    "import json\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with open(output_dir + \"/tokenizer.json\", 'w') as json_file:\n",
    "    json.dump(tokenizer_json, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current CUDA device index: 0\n",
      "Current CUDA device name: NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the current CUDA device index\n",
    "    current_device = torch.cuda.current_device()\n",
    "    print(f\"Current CUDA device index: {current_device}\")\n",
    "\n",
    "    # Get the name of the current CUDA device\n",
    "    current_device_name = torch.cuda.get_device_name(current_device)\n",
    "    print(f\"Current CUDA device name: {current_device_name}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBartForSequenceClassification: ['model.decoder.version', 'model.encoder.version']\n",
      "- This IS expected if you are initializing TFBartForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBartForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBartForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBartForSequenceClassification, BartTokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "# Assign the model and tokenizer\n",
    "model = TFBartForSequenceClassification.from_pretrained(\"facebook/bart-large-mnli\")\n",
    "model.config.dropout = 0.5\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model.classification_head.dense = Dense(model.config.d_model, activation='linear', use_bias=True)\n",
    "model.classification_head.out_proj = Dense(5, activation='linear', use_bias=True)\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-mnli\",  return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at C:/Users/aryan/Actual-Coding/CDAC/patents-output/bart were not used when initializing TFBartForSequenceClassification: ['final_logits_bias']\n",
      "- This IS expected if you are initializing TFBartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBartForSequenceClassification were not initialized from the model checkpoint at C:/Users/aryan/Actual-Coding/CDAC/patents-output/bart and are newly initialized: ['classification_head']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "loaded_model = model.from_pretrained('C:/Users/aryan/Actual-Coding/CDAC/patents-output/bart') # type: ignore\n",
    "loaded_tokenizer = tokenizer.from_pretrained('C:/Users/aryan/Actual-Coding/CDAC/patents-output/bart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "premise = \"ai powered multi stage adjustment\"\n",
    "hypothesis = \"smart water filtration device\"\n",
    "input_ids = tokenizer(premise, hypothesis, truncation=True, padding=True, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=int64, numpy=array([2], dtype=int64)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = loaded_model(input_ids)\n",
    "logits = outputs.logits\n",
    "\n",
    "probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "predicted_class = tf.argmax(probabilities, axis=-1)\n",
    "predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = tf.linspace(1.0, 3.0, num=3)\n",
    "expected_score = tf.reduce_sum(probabilities * scores, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score = 3.0\n",
    "normalized_score = expected_score / max_score\n",
    "rounded_score = tf.round(normalized_score * 4) / 4\n",
    "clamped_score = tf.minimum(rounded_score, tf.constant(1.00))\n",
    "\n",
    "formatted_output = clamped_score.numpy()\n",
    "formatted_output_str = [\"{:.2f}\".format(float(score)) for score in formatted_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9221207499504089\n",
      "Unrelated\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "score_to_label_mapping = {\n",
    "    0.00: \"Very close match\",\n",
    "    0.25: \"Close synonym\",\n",
    "    0.50: \"Synonyms which donâ€™t have the same meaning (same function, same properties)\",\n",
    "    0.75: \"Somewhat related\",\n",
    "    1.00: \"Unrelated\"\n",
    "}\n",
    "\n",
    "# Make sure to convert the numpy array to a float\n",
    "rounded_score_value = float(rounded_score.numpy()[0])\n",
    "print(float(normalized_score.numpy()[0]))\n",
    "\n",
    "# You don't need to format it as a string, use the float value directly for lookup\n",
    "label = score_to_label_mapping.get(rounded_score_value, \"Label not found\")\n",
    "print(label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
