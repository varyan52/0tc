{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-large and are newly initialized: ['pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from transformers import DebertaTokenizer, DebertaForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-large\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "model = DebertaForSequenceClassification.from_pretrained(\"microsoft/deberta-large\", num_labels=5)\n",
    "\n",
    "model.config.dropout = 0.5\n",
    "model.classifier = torch.nn.Linear(in_features=model.config.hidden_size, out_features=5, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_dataset = pd.DataFrame(pd.read_csv('/Users/aryan/Actual-Coding/CDAC/us-patent-phrase-to-phrase-matching/train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mapping = {\n",
    "    0: 0,\n",
    "    0.25: 1,\n",
    "    0.5: 2,\n",
    "    0.75: 3,\n",
    "    1: 4\n",
    "}\n",
    "\n",
    "train_dataset['score'] = train_dataset['score'].map(score_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_dataset, cross_verify_data = train_test_split(train_dataset, test_size=0.1)  # 10% for cross-verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from random import randint\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def replace_synonym(sentence, num_replacements=1):\n",
    "    words = sentence.split()\n",
    "    new_sentence = sentence\n",
    "    for _ in range(num_replacements):\n",
    "        word_to_replace = words[randint(0, len(words)-1)]\n",
    "        synonyms = [syn.name().split('.')[0] for syn in wordnet.synsets(word_to_replace) if syn.name().split('.')[0] != word_to_replace]\n",
    "        if synonyms:\n",
    "            new_sentence = new_sentence.replace(word_to_replace, synonyms[0], 1)\n",
    "    return new_sentence\n",
    "\n",
    "# Augmenting data\n",
    "df_train_dataset['target'] = df_train_dataset['target'].apply(lambda x: replace_synonym(x, num_replacements=5))\n",
    "df_train_dataset['anchor'] = df_train_dataset['anchor'].apply(lambda x: replace_synonym(x, num_replacements=5))\n",
    "cross_verify_data['target'] = cross_verify_data['target'].apply(lambda x: replace_synonym(x, num_replacements=5))\n",
    "cross_verify_data['anchor'] = cross_verify_data['anchor'].apply(lambda x: replace_synonym(x, num_replacements=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score\n",
       "2        20\n",
       "1        18\n",
       "0         8\n",
       "3         4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_actual = df_train_dataset['score']\n",
    "score_list = score_actual[1:51].tolist()\n",
    "score_train = pd.DataFrame(score_actual[1:51])\n",
    "score_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ammonia convalescence',\n",
       " 'inside close',\n",
       " 'grow dilute layer',\n",
       " 'analogue predilection',\n",
       " 'dimensional placement',\n",
       " 'comfortability',\n",
       " 'receiver shot',\n",
       " 'forcible conveyance culture_culture_medium',\n",
       " 'rip into flow',\n",
       " 'oxygen aircraft_aircraft_carrier',\n",
       " 'decrease component',\n",
       " 'antiatherosclerotic',\n",
       " 'yoke weaponry',\n",
       " 'predetermine acceleration',\n",
       " 'field_field_glass region',\n",
       " 'aesthetic effects',\n",
       " 'paroxysm inside margin',\n",
       " 'remain downstairs doorway prize',\n",
       " 'function internal burning locomotive',\n",
       " 'poll',\n",
       " 'estimate sum',\n",
       " 'grow dilute layer',\n",
       " 'crystalline melted quartz_glass expose',\n",
       " 'axle box',\n",
       " 'breech mechanism',\n",
       " 'wall military_military_military_military_post',\n",
       " 'further extra component',\n",
       " 'pen establish calculator',\n",
       " 'display_panel idahoaho',\n",
       " 'datum end_product circuit',\n",
       " 'plane provision curler',\n",
       " 'faucet fabrication',\n",
       " 'animal fat',\n",
       " 'ack',\n",
       " 'graphic persona',\n",
       " 'inside close',\n",
       " 'angular liaison bearing',\n",
       " 'debar luminosity',\n",
       " 'aide body_of_body_of_body_of_water',\n",
       " 'energizing military_military_military_position',\n",
       " 'display isotropy',\n",
       " 'forest article',\n",
       " 'beginning electric_potential',\n",
       " 'fill inside',\n",
       " 'remedy application',\n",
       " 'feel actuator',\n",
       " 'kind arsenic torso',\n",
       " 'victor grinding',\n",
       " 'channel vector',\n",
       " 'passage extremity']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_dataset['hypothesis'] = df_train_dataset['anchor']\n",
    "inputs = df_train_dataset['hypothesis']\n",
    "hypothesis_list = inputs[1:51].tolist()\n",
    "hypothesis_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['convalescence of body_of_water',\n",
       " 'cylindrical inside extremity',\n",
       " 'grow layer',\n",
       " 'nothing slant',\n",
       " 'teleteletelephone mise_en_scene',\n",
       " 'undertaking',\n",
       " 'replaceable shot fabrication',\n",
       " 'forcible chemistry',\n",
       " 'burrow',\n",
       " 'metallic_elementlic_elementlic_element procedure',\n",
       " 'argument',\n",
       " 'cholesterol lower',\n",
       " 'center_field connection',\n",
       " 'doorway acceleration',\n",
       " 'built-in home percolate',\n",
       " 'semblance mix',\n",
       " 'disposable cup',\n",
       " 'aim control device',\n",
       " 'stuff corner',\n",
       " 'place poll',\n",
       " 'sum refund',\n",
       " 'produce dilute movie',\n",
       " 'campaign quartz_glass expose device',\n",
       " 'axle spring',\n",
       " 'plastic organ_organ_organ_pipe',\n",
       " 'bill_bill_bill_poster devising',\n",
       " 'extra home_plate component',\n",
       " 'playplayplayplaypen pad',\n",
       " 'faculty designation datum',\n",
       " 'memory device',\n",
       " 'rotatable plane supply curler',\n",
       " 'faucet',\n",
       " 'animal menagerie',\n",
       " 'datum network',\n",
       " 'calculator graphic persona',\n",
       " 'inwardly inside axile',\n",
       " 'plurality of peal elements',\n",
       " 'variety the luminosity',\n",
       " 'body_of_body_of_water go_around device',\n",
       " 'distal widen state_of_matter',\n",
       " 'symmetricalal affair',\n",
       " 'wooden substrate',\n",
       " 'doorway',\n",
       " 'filled along the inwardly',\n",
       " 'cream lotion',\n",
       " 'credit_card melted strainer',\n",
       " 'kind arsenic ball extremity',\n",
       " 'clothing',\n",
       " 'groove system_of_system_of_system_of_system_of_weightssss vector',\n",
       " 'passage region']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "premise = df_train_dataset['target']\n",
    "premise_list = premise[1:51].tolist()\n",
    "premise_for_testing = pd.DataFrame(premise_list)\n",
    "premise_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "input_ids = tokenizer(premise_list, hypothesis_list, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "attention_masks = input_ids[\"attention_mask\"]\n",
    "labels = torch.tensor(score_list, dtype=torch.long)\n",
    "labels_one_hot = torch.nn.functional.one_hot(labels, num_classes=5)\n",
    "labels_one_hot = labels_one_hot.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Up Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encoded_texts, labels):\n",
    "        self.encoded_texts = encoded_texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encoded_texts.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "train_dataset = CustomDataset(input_ids, labels_one_hot)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02fab241391b4921947387b00b75fb2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 22.7636, 'train_samples_per_second': 2.196, 'train_steps_per_second': 0.439, 'train_loss': 16.08875274658203, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=16.08875274658203, metrics={'train_runtime': 22.7636, 'train_samples_per_second': 2.196, 'train_steps_per_second': 0.439, 'train_loss': 16.08875274658203, 'epoch': 1.0})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./patents-output\",\n",
    "    per_device_train_batch_size = 5,\n",
    "    num_train_epochs = 1,\n",
    "    learning_rate = 1e-2,\n",
    "    save_steps = 500,\n",
    "    save_total_limit = 2,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps = 500,\n",
    "    max_grad_norm=1.0,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model, # type: ignore\n",
    "    args = training_args,\n",
    "    data_collator = data_collator,\n",
    "    train_dataset = train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_verify_hypothesis = cross_verify_data['anchor'].tolist()\n",
    "cross_verify_premise = cross_verify_data['target'].tolist()\n",
    "cross_verify_score = cross_verify_data['score'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_verify_input_ids = tokenizer(cross_verify_premise, cross_verify_hypothesis, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "cross_verify_score_tensor = torch.tensor(cross_verify_score, dtype=torch.long)\n",
    "cross_verify_labels_one_hot = torch.nn.functional.one_hot(cross_verify_score_tensor, num_classes=5).float()\n",
    "cross_verify_dataset = CustomDataset(cross_verify_input_ids, cross_verify_labels_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a89ad5a5da40c392f3c91666545323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "cross_verify_results = trainer.predict(cross_verify_dataset)\n",
    "cross_verify_predictions = cross_verify_results.predictions\n",
    "cross_verify_label_ids = cross_verify_results.label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       762\n",
      "           1       1.00      1.00      1.00      1146\n",
      "           2       1.00      1.00      1.00      1199\n",
      "           3       1.00      1.00      1.00       435\n",
      "           4       1.00      1.00      1.00       106\n",
      "\n",
      "   micro avg       1.00      1.00      1.00      3648\n",
      "   macro avg       1.00      1.00      1.00      3648\n",
      "weighted avg       1.00      1.00      1.00      3648\n",
      " samples avg       1.00      1.00      1.00      3648\n",
      "\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(classification_report(cross_verify_labels_one_hot, cross_verify_label_ids)) # type: ignore\n",
    "print(\"Accuracy:\", accuracy_score(cross_verify_labels_one_hot, cross_verify_label_ids)) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: id\n",
      "True Label: tensor([1., 0., 0., 0., 0.])\n",
      "Predicted Label: [1. 0. 0. 0. 0.]\n",
      "--------------------------------------------------\n",
      "Input: anchor\n",
      "True Label: tensor([0., 1., 0., 0., 0.])\n",
      "Predicted Label: [0. 1. 0. 0. 0.]\n",
      "--------------------------------------------------\n",
      "Input: target\n",
      "True Label: tensor([0., 1., 0., 0., 0.])\n",
      "Predicted Label: [0. 1. 0. 0. 0.]\n",
      "--------------------------------------------------\n",
      "Input: context\n",
      "True Label: tensor([0., 0., 0., 1., 0.])\n",
      "Predicted Label: [0. 0. 0. 1. 0.]\n",
      "--------------------------------------------------\n",
      "Input: score\n",
      "True Label: tensor([0., 0., 1., 0., 0.])\n",
      "Predicted Label: [0. 0. 1. 0. 0.]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for input, true_label, predicted_label in zip(cross_verify_data[:10], cross_verify_labels_one_hot[:10], cross_verify_label_ids[:10]): # type: ignore\n",
    "    print(f\"Input: {input}\")\n",
    "    print(f\"True Label: {true_label}\")\n",
    "    print(f\"Predicted Label: {predicted_label}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./patents-output/deberta\\\\tokenizer_config.json',\n",
       " './patents-output/deberta\\\\special_tokens_map.json',\n",
       " './patents-output/deberta\\\\vocab.json',\n",
       " './patents-output/deberta\\\\merges.txt',\n",
       " './patents-output/deberta\\\\added_tokens.json')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = \"./patents-output/deberta\"\n",
    "\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDebertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFDebertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDebertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\deberta\\modeling_tf_deberta.py:123: Bernoulli.__init__ (from tensorflow.python.ops.distributions.bernoulli) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From c:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\distributions\\bernoulli.py:86: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, dense_layer_call_fn while saving (showing 5 of 924). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./patents-output/deberta\\saved_model\\1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./patents-output/deberta\\saved_model\\1\\assets\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(output_dir)\n",
    "tf_model = TFAutoModelForSequenceClassification.from_pretrained(output_dir, from_pt=True, config=config)\n",
    "tf_model.save_pretrained(output_dir, saved_model=True)\n",
    "tf_model.save_weights(output_dir + '/tf_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_model = model.from_pretrained('./patents-output/bart')\n",
    "#loaded_tokenizer = tokenizer.from_pretrained('./patents-output/bart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current CUDA device index: 0\n",
      "Current CUDA device name: NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the current CUDA device index\n",
    "    current_device = torch.cuda.current_device()\n",
    "    print(f\"Current CUDA device index: {current_device}\")\n",
    "\n",
    "    # Get the name of the current CUDA device\n",
    "    current_device_name = torch.cuda.get_device_name(current_device)\n",
    "    print(f\"Current CUDA device name: {current_device_name}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBartForSequenceClassification: ['model.decoder.version', 'model.encoder.version']\n",
      "- This IS expected if you are initializing TFBartForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBartForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBartForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBartForSequenceClassification, BartTokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "# Assign the model and tokenizer\n",
    "model = TFBartForSequenceClassification.from_pretrained(\"facebook/bart-large-mnli\")\n",
    "model.config.dropout = 0.5\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model.classification_head.dense = Dense(model.config.d_model, activation='linear', use_bias=True)\n",
    "model.classification_head.out_proj = Dense(5, activation='linear', use_bias=True)\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-mnli\",  return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at C:/Users/aryan/Actual-Coding/CDAC/patents-output/bart were not used when initializing TFBartForSequenceClassification: ['final_logits_bias']\n",
      "- This IS expected if you are initializing TFBartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBartForSequenceClassification were not initialized from the model checkpoint at C:/Users/aryan/Actual-Coding/CDAC/patents-output/bart and are newly initialized: ['classification_head']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "loaded_model = model.from_pretrained('C:/Users/aryan/Actual-Coding/CDAC/patents-output/bart') # type: ignore\n",
    "loaded_tokenizer = tokenizer.from_pretrained('C:/Users/aryan/Actual-Coding/CDAC/patents-output/bart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "premise = \"ai powered multi stage adjustment\"\n",
    "hypothesis = \"smart water filtration device\"\n",
    "input_ids = tokenizer(premise, hypothesis, truncation=True, padding=True, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=int64, numpy=array([2], dtype=int64)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = loaded_model(input_ids)\n",
    "logits = outputs.logits\n",
    "\n",
    "probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "predicted_class = tf.argmax(probabilities, axis=-1)\n",
    "predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = tf.linspace(1.0, 3.0, num=3)\n",
    "expected_score = tf.reduce_sum(probabilities * scores, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score = 3.0\n",
    "normalized_score = expected_score / max_score\n",
    "rounded_score = tf.round(normalized_score * 4) / 4\n",
    "clamped_score = tf.minimum(rounded_score, tf.constant(1.00))\n",
    "\n",
    "formatted_output = clamped_score.numpy()\n",
    "formatted_output_str = [\"{:.2f}\".format(float(score)) for score in formatted_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7048661708831787\n",
      "Somewhat related\n"
     ]
    }
   ],
   "source": [
    "score_to_label_mapping = {\n",
    "    0.00: \"Very close match\",\n",
    "    0.25: \"Close synonym\",\n",
    "    0.50: \"Synonyms which don’t have the same meaning (same function, same properties)\",\n",
    "    0.75: \"Somewhat related\",\n",
    "    1.00: \"Unrelated\"\n",
    "}\n",
    "\n",
    "# Make sure to convert the numpy array to a float\n",
    "rounded_score_value = float(rounded_score.numpy()[0])\n",
    "print(float(normalized_score.numpy()[0]))\n",
    "\n",
    "# You don't need to format it as a string, use the float value directly for lookup\n",
    "label = score_to_label_mapping.get(rounded_score_value, \"Label not found\")\n",
    "print(label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
