{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current CUDA device index: 0\n",
      "Current CUDA device name: NVIDIA RTX A4000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the current CUDA device index\n",
    "    current_device = torch.cuda.current_device()\n",
    "    print(f\"Current CUDA device index: {current_device}\")\n",
    "\n",
    "    # Get the name of the current CUDA device\n",
    "    current_device_name = torch.cuda.get_device_name(current_device)\n",
    "    print(f\"Current CUDA device name: {current_device_name}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n",
      "<module 'tensorflow._api.v2.data' from 'c:\\\\Users\\\\cl502_20\\\\Downloads\\\\Vishrut Aryan\\\\tf-venv\\\\lib\\\\site-packages\\\\tensorflow\\\\_api\\\\v2\\\\data\\\\__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.data)  # This should not raise any error if TensorFlow is correctly installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from transformers import BartForSequenceClassification\n",
    "from transformers import BartTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-mnli\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "model = BartForSequenceClassification.from_pretrained(\"facebook/bart-large-mnli\")\n",
    "model.config.dropout = 0.5\n",
    "model.classification_head.dense = torch.nn.Linear(in_features=model.config.d_model, out_features=model.config.d_model, bias=True)\n",
    "model.classification_head.out_proj = torch.nn.Linear(in_features=model.config.d_model, out_features=5, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_dataset = pd.DataFrame(pd.read_csv('train_patents.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mapping = {\n",
    "    0: 0,\n",
    "    0.25: 1,\n",
    "    0.5: 2,\n",
    "    0.75: 3,\n",
    "    1: 4\n",
    "}\n",
    "\n",
    "train_dataset['score'] = train_dataset['score'].map(score_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_dataset, cross_verify_data = train_test_split(train_dataset, test_size=0.4)  # 10% for cross-verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cl502_20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from random import randint\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def replace_synonym(sentence, num_replacements=1):\n",
    "    words = sentence.split()\n",
    "    new_sentence = sentence\n",
    "    for _ in range(num_replacements):\n",
    "        word_to_replace = words[randint(0, len(words)-1)]\n",
    "        synonyms = [syn.name().split('.')[0] for syn in wordnet.synsets(word_to_replace) if syn.name().split('.')[0] != word_to_replace]\n",
    "        if synonyms:\n",
    "            new_sentence = new_sentence.replace(word_to_replace, synonyms[0], 1)\n",
    "    return new_sentence\n",
    "\n",
    "# Augmenting data\n",
    "df_train_dataset['target'] = df_train_dataset['target'].apply(lambda x: replace_synonym(x, num_replacements=5))\n",
    "df_train_dataset['anchor'] = df_train_dataset['anchor'].apply(lambda x: replace_synonym(x, num_replacements=5))\n",
    "cross_verify_data['target'] = cross_verify_data['target'].apply(lambda x: replace_synonym(x, num_replacements=5))\n",
    "cross_verify_data['anchor'] = cross_verify_data['anchor'].apply(lambda x: replace_synonym(x, num_replacements=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_actual = df_train_dataset['score']\n",
    "score_list = score_actual[1:10001].tolist()\n",
    "eval_score_list = score_actual[10001:12001].tolist()\n",
    "score_train = pd.DataFrame(score_actual[1:10001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score\n",
       "2        3284\n",
       "1        3226\n",
       "0        2038\n",
       "3        1134\n",
       "4         318\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15837</th>\n",
       "      <td>058ede162eeabb86</td>\n",
       "      <td>loanblend arrangement</td>\n",
       "      <td>security arrangement</td>\n",
       "      <td>C07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3115</th>\n",
       "      <td>075aa525d34a1e42</td>\n",
       "      <td>barrage cellular_telephoneular_telephoneular_t...</td>\n",
       "      <td>electromechanical apparatus</td>\n",
       "      <td>F28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9242</th>\n",
       "      <td>748a3d3c9a7fae14</td>\n",
       "      <td>dimensional placement</td>\n",
       "      <td>software mise_en_scene</td>\n",
       "      <td>E02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19425</th>\n",
       "      <td>2573613d8f2dca98</td>\n",
       "      <td>fabric constitution</td>\n",
       "      <td>component constitution</td>\n",
       "      <td>A21</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35050</th>\n",
       "      <td>6af7ba75cc53d0e4</td>\n",
       "      <td>vibratory actuator</td>\n",
       "      <td>vibratory stadium</td>\n",
       "      <td>A61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24457</th>\n",
       "      <td>93f73962920dc83b</td>\n",
       "      <td>organ_pipe corner</td>\n",
       "      <td>hosiery organ_pipe covering</td>\n",
       "      <td>C21</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12637</th>\n",
       "      <td>e41129c3173b73e0</td>\n",
       "      <td>run control_condition valve</td>\n",
       "      <td>stream complete valve</td>\n",
       "      <td>F25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32303</th>\n",
       "      <td>6f743cd5e277e359</td>\n",
       "      <td>acme</td>\n",
       "      <td>crest extremum</td>\n",
       "      <td>B65</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21344</th>\n",
       "      <td>c1945e591ab7177e</td>\n",
       "      <td>nvm range</td>\n",
       "      <td>memory range</td>\n",
       "      <td>G11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32893</th>\n",
       "      <td>0d18b7867c0e8de0</td>\n",
       "      <td>terephthalate polyester</td>\n",
       "      <td>naphthalate</td>\n",
       "      <td>D06</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                             anchor  \\\n",
       "15837  058ede162eeabb86                              loanblend arrangement   \n",
       "3115   075aa525d34a1e42  barrage cellular_telephoneular_telephoneular_t...   \n",
       "9242   748a3d3c9a7fae14                              dimensional placement   \n",
       "19425  2573613d8f2dca98                                fabric constitution   \n",
       "35050  6af7ba75cc53d0e4                                 vibratory actuator   \n",
       "24457  93f73962920dc83b                                  organ_pipe corner   \n",
       "12637  e41129c3173b73e0                        run control_condition valve   \n",
       "32303  6f743cd5e277e359                                               acme   \n",
       "21344  c1945e591ab7177e                                          nvm range   \n",
       "32893  0d18b7867c0e8de0                            terephthalate polyester   \n",
       "\n",
       "                            target context  score  \n",
       "15837         security arrangement     C07      0  \n",
       "3115   electromechanical apparatus     F28      2  \n",
       "9242        software mise_en_scene     E02      0  \n",
       "19425       component constitution     A21      3  \n",
       "35050            vibratory stadium     A61      0  \n",
       "24457  hosiery organ_pipe covering     C21      2  \n",
       "12637        stream complete valve     F25      2  \n",
       "32303               crest extremum     B65      3  \n",
       "21344                 memory range     G11      2  \n",
       "32893                  naphthalate     D06      2  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_dataset['hypothesis'] = df_train_dataset['anchor']\n",
    "inputs = df_train_dataset['hypothesis']\n",
    "hypothesis_list = inputs[1:10001].tolist()\n",
    "eval_hypothesis_list = inputs[10001:12001].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "premise = df_train_dataset['target']\n",
    "premise_list = premise[1:10001].tolist()\n",
    "eval_premise_list = premise[10001:12001].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "input_ids = tokenizer(premise_list, hypothesis_list, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "attention_masks = input_ids[\"attention_mask\"]\n",
    "labels = torch.tensor(score_list, dtype=torch.long)\n",
    "labels_one_hot = torch.nn.functional.one_hot(labels, num_classes=5)\n",
    "labels_one_hot = labels_one_hot.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Up Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encoded_texts, attention_masks, labels):\n",
    "        self.encoded_texts = encoded_texts\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encoded_texts.items()}\n",
    "        item[\"attention_mask\"] = self.attention_masks[idx]\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "train_dataset = CustomDataset(input_ids, attention_masks, labels_one_hot)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_input_ids = tokenizer(eval_premise_list, eval_hypothesis_list, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "eval_attention_masks = eval_input_ids[\"attention_mask\"]\n",
    "eval_labels = torch.tensor(eval_score_list, dtype=torch.long)\n",
    "eval_labels_one_hot = torch.nn.functional.one_hot(eval_labels, num_classes=5)\n",
    "eval_labels_one_hot = eval_labels_one_hot.float()\n",
    "\n",
    "eval_dataset = CustomDataset(eval_input_ids, eval_attention_masks, eval_labels_one_hot)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbba700b6ea945769c31fe01d1360e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./patents-output\",\n",
    "    per_device_train_batch_size = 5,\n",
    "    num_train_epochs = 3,\n",
    "    learning_rate = 1e-2,\n",
    "    save_steps = 500,\n",
    "    save_total_limit = 2,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps = 500,\n",
    "    max_grad_norm=1.0,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model, # type: ignore\n",
    "    args = training_args,\n",
    "    data_collator = data_collator,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    #eval_dataloader = eval_dataloader,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_verify_hypothesis = cross_verify_data['anchor'].tolist()\n",
    "cross_verify_premise = cross_verify_data['target'].tolist()\n",
    "cross_verify_score = cross_verify_data['score'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_verify_input_ids = tokenizer(cross_verify_premise, cross_verify_hypothesis, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "cross_verify_score_tensor = torch.tensor(cross_verify_score, dtype=torch.long)\n",
    "cross_verify_labels_one_hot = torch.nn.functional.one_hot(cross_verify_score_tensor, num_classes=5).float()\n",
    "cross_verify_dataset = CustomDataset(cross_verify_input_ids, cross_verify_labels_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "005562b88c074090b0c51b2754e63c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "cross_verify_results = trainer.predict(cross_verify_dataset)\n",
    "cross_verify_predictions = cross_verify_results.predictions\n",
    "cross_verify_label_ids = cross_verify_results.label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       769\n",
      "           1       1.00      1.00      1.00      1154\n",
      "           2       1.00      1.00      1.00      1209\n",
      "           3       1.00      1.00      1.00       397\n",
      "           4       1.00      1.00      1.00       119\n",
      "\n",
      "   micro avg       1.00      1.00      1.00      3648\n",
      "   macro avg       1.00      1.00      1.00      3648\n",
      "weighted avg       1.00      1.00      1.00      3648\n",
      " samples avg       1.00      1.00      1.00      3648\n",
      "\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(classification_report(cross_verify_labels_one_hot, cross_verify_label_ids)) # type: ignore\n",
    "print(\"Accuracy:\", accuracy_score(cross_verify_labels_one_hot, cross_verify_label_ids)) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: id\n",
      "True Label: tensor([1., 0., 0., 0., 0.])\n",
      "Predicted Label: [1. 0. 0. 0. 0.]\n",
      "--------------------------------------------------\n",
      "Input: anchor\n",
      "True Label: tensor([0., 1., 0., 0., 0.])\n",
      "Predicted Label: [0. 1. 0. 0. 0.]\n",
      "--------------------------------------------------\n",
      "Input: target\n",
      "True Label: tensor([1., 0., 0., 0., 0.])\n",
      "Predicted Label: [1. 0. 0. 0. 0.]\n",
      "--------------------------------------------------\n",
      "Input: context\n",
      "True Label: tensor([0., 0., 1., 0., 0.])\n",
      "Predicted Label: [0. 0. 1. 0. 0.]\n",
      "--------------------------------------------------\n",
      "Input: score\n",
      "True Label: tensor([0., 0., 0., 1., 0.])\n",
      "Predicted Label: [0. 0. 0. 1. 0.]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for input, true_label, predicted_label in zip(cross_verify_data[:10], cross_verify_labels_one_hot[:10], cross_verify_label_ids[:10]): # type: ignore\n",
    "    print(f\"Input: {input}\")\n",
    "    print(f\"True Label: {true_label}\")\n",
    "    print(f\"Predicted Label: {predicted_label}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BartForSequenceClassification' object has no attribute 'save_weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\cl502_20\\Downloads\\Vishrut Aryan\\zeroshot_patents_bart.ipynb Cell 26\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cl502_20/Downloads/Vishrut%20Aryan/zeroshot_patents_bart.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39msave_pretrained(output_dir)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cl502_20/Downloads/Vishrut%20Aryan/zeroshot_patents_bart.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m tokenizer\u001b[39m.\u001b[39msave_pretrained(output_dir)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/cl502_20/Downloads/Vishrut%20Aryan/zeroshot_patents_bart.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39;49msave_weights(output_dir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/bart_model.h5\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\cl502_20\\Downloads\\Vishrut Aryan\\tf-venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1693\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1694\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1695\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BartForSequenceClassification' object has no attribute 'save_weights'"
     ]
    }
   ],
   "source": [
    "output_dir = \"./patents-output/bart\"\n",
    "\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "model.save_weights(output_dir + '/bart_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '\\x10'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\cl502_20\\Downloads\\Vishrut Aryan\\zeroshot_patents_bart.ipynb Cell 27\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cl502_20/Downloads/Vishrut%20Aryan/zeroshot_patents_bart.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoConfig\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cl502_20/Downloads/Vishrut%20Aryan/zeroshot_patents_bart.ipynb#X34sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m config \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39mfrom_pretrained(output_dir)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/cl502_20/Downloads/Vishrut%20Aryan/zeroshot_patents_bart.ipynb#X34sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m tf_model \u001b[39m=\u001b[39m TFAutoModelForSequenceClassification\u001b[39m.\u001b[39;49mfrom_pretrained(output_dir, from_pt\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, config\u001b[39m=\u001b[39;49mconfig)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cl502_20/Downloads/Vishrut%20Aryan/zeroshot_patents_bart.ipynb#X34sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m tf_model\u001b[39m.\u001b[39msave_pretrained(output_dir, saved_model\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, use_auth_token\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cl502_20/Downloads/Vishrut%20Aryan/zeroshot_patents_bart.ipynb#X34sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m tf_model\u001b[39m.\u001b[39msave_weights(output_dir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/tf_model.h5\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\cl502_20\\Downloads\\Vishrut Aryan\\tf-venv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    565\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 566\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    567\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39mmodel_args, config\u001b[39m=\u001b[39mconfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    568\u001b[0m     )\n\u001b[0;32m    569\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    570\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    571\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    572\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\cl502_20\\Downloads\\Vishrut Aryan\\tf-venv\\lib\\site-packages\\transformers\\modeling_tf_utils.py:2898\u001b[0m, in \u001b[0;36mTFPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2895\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodeling_tf_pytorch_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m load_pytorch_checkpoint_in_tf2_model\n\u001b[0;32m   2897\u001b[0m     \u001b[39m# Load from a PyTorch checkpoint\u001b[39;00m\n\u001b[1;32m-> 2898\u001b[0m     \u001b[39mreturn\u001b[39;00m load_pytorch_checkpoint_in_tf2_model(\n\u001b[0;32m   2899\u001b[0m         model,\n\u001b[0;32m   2900\u001b[0m         resolved_archive_file,\n\u001b[0;32m   2901\u001b[0m         allow_missing_keys\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   2902\u001b[0m         output_loading_info\u001b[39m=\u001b[39;49moutput_loading_info,\n\u001b[0;32m   2903\u001b[0m         _prefix\u001b[39m=\u001b[39;49mload_weight_prefix,\n\u001b[0;32m   2904\u001b[0m         tf_to_pt_weight_rename\u001b[39m=\u001b[39;49mtf_to_pt_weight_rename,\n\u001b[0;32m   2905\u001b[0m     )\n\u001b[0;32m   2907\u001b[0m \u001b[39m# we might need to extend the variable scope for composite models\u001b[39;00m\n\u001b[0;32m   2908\u001b[0m \u001b[39mif\u001b[39;00m load_weight_prefix \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\cl502_20\\Downloads\\Vishrut Aryan\\tf-venv\\lib\\site-packages\\transformers\\modeling_tf_pytorch_utils.py:185\u001b[0m, in \u001b[0;36mload_pytorch_checkpoint_in_tf2_model\u001b[1;34m(tf_model, pytorch_checkpoint_path, tf_inputs, allow_missing_keys, output_loading_info, _prefix, tf_to_pt_weight_rename)\u001b[0m\n\u001b[0;32m    183\u001b[0m     pt_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(path)\n\u001b[0;32m    184\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoading PyTorch weights from \u001b[39m\u001b[39m{\u001b[39;00mpt_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 185\u001b[0m     pt_state_dict\u001b[39m.\u001b[39mupdate(torch\u001b[39m.\u001b[39;49mload(pt_path, map_location\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m    187\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPyTorch checkpoint contains \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39msum\u001b[39m(t\u001b[39m.\u001b[39mnumel()\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mt\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39mpt_state_dict\u001b[39m.\u001b[39mvalues())\u001b[39m:\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m parameters\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    189\u001b[0m \u001b[39mreturn\u001b[39;00m load_pytorch_weights_in_tf2_model(\n\u001b[0;32m    190\u001b[0m     tf_model,\n\u001b[0;32m    191\u001b[0m     pt_state_dict,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    196\u001b[0m     tf_to_pt_weight_rename\u001b[39m=\u001b[39mtf_to_pt_weight_rename,\n\u001b[0;32m    197\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\cl502_20\\Downloads\\Vishrut Aryan\\tf-venv\\lib\\site-packages\\torch\\serialization.py:1028\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1026\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1027\u001b[0m         \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1028\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n",
      "File \u001b[1;32mc:\\Users\\cl502_20\\Downloads\\Vishrut Aryan\\tf-venv\\lib\\site-packages\\torch\\serialization.py:1246\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1240\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(f, \u001b[39m'\u001b[39m\u001b[39mreadinto\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m (\u001b[39m3\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mversion_info \u001b[39m<\u001b[39m (\u001b[39m3\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m2\u001b[39m):\n\u001b[0;32m   1241\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1242\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtorch.load does not work with file-like objects that do not implement readinto on Python 3.8.0 and 3.8.1. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1243\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReceived object of type \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(f)\u001b[39m}\u001b[39;00m\u001b[39m\\\"\u001b[39;00m\u001b[39m. Please update to Python 3.8.2 or newer to restore this \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1244\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfunctionality.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1246\u001b[0m magic_number \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mload(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1247\u001b[0m \u001b[39mif\u001b[39;00m magic_number \u001b[39m!=\u001b[39m MAGIC_NUMBER:\n\u001b[0;32m   1248\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid magic number; corrupt file?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: invalid load key, '\\x10'."
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(output_dir)\n",
    "tf_model = TFAutoModelForSequenceClassification.from_pretrained(output_dir, from_pt=True, config=config)\n",
    "tf_model.save_pretrained(output_dir, saved_model=True, use_auth_token=False)\n",
    "tf_model.save_weights(output_dir + '/tf_bart_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_model = model.from_pretrained('./patents-output/deberta')\n",
    "#loaded_tokenizer = tokenizer.from_pretrained('./patents-output/deberta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as serving, model.shared_layer_call_fn, model.shared_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses while saving (showing 5 of 817). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./patents-output/bart\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./patents-output/bart\\assets\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BartTokenizer' object has no attribute 'to_json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\cl502_20\\Downloads\\Vishrut Aryan\\zeroshot_patents_bart.ipynb Cell 29\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cl502_20/Downloads/Vishrut%20Aryan/zeroshot_patents_bart.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39msave(output_dir)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cl502_20/Downloads/Vishrut%20Aryan/zeroshot_patents_bart.ipynb#X36sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/cl502_20/Downloads/Vishrut%20Aryan/zeroshot_patents_bart.ipynb#X36sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m tokenizer_json \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mto_json()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cl502_20/Downloads/Vishrut%20Aryan/zeroshot_patents_bart.ipynb#X36sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(output_dir \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/tokenizer.json\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m json_file:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cl502_20/Downloads/Vishrut%20Aryan/zeroshot_patents_bart.ipynb#X36sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     json\u001b[39m.\u001b[39mdump(tokenizer_json, json_file)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BartTokenizer' object has no attribute 'to_json'"
     ]
    }
   ],
   "source": [
    "model.save(output_dir)\n",
    "\n",
    "import json\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with open(output_dir + \"/tokenizer.json\", 'w') as json_file:\n",
    "    json.dump(tokenizer_json, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBartForSequenceClassification: ['model.encoder.version', 'model.decoder.version']\n",
      "- This IS expected if you are initializing TFBartForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBartForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBartForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBartForSequenceClassification, BartTokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "# Initialize the model\n",
    "model = TFBartForSequenceClassification.from_pretrained(\"facebook/bart-large-mnli\")\n",
    "model.config.dropout = 0.5\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "model.classification_head.dense = Dense(model.config.d_model, activation='linear', use_bias=True)\n",
    "model.classification_head.out_proj = Dense(5, activation='linear', use_bias=True)\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'op' and 'message'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\cl502_20\\Downloads\\Vishrut Aryan\\tf-venv\\lib\\site-packages\\transformers\\modeling_tf_pytorch_utils.py:333\u001b[0m, in \u001b[0;36mload_pytorch_state_dict_in_tf2_model\u001b[1;34m(tf_model, pt_state_dict, tf_inputs, allow_missing_keys, output_loading_info, _prefix, tf_to_pt_weight_rename, ignore_mismatched_sizes)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 333\u001b[0m     array \u001b[39m=\u001b[39m apply_transpose(transpose, array, symbolic_weight\u001b[39m.\u001b[39;49mshape)\n\u001b[0;32m    334\u001b[0m \u001b[39mexcept\u001b[39;00m tf\u001b[39m.\u001b[39merrors\u001b[39m.\u001b[39mInvalidArgumentError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\cl502_20\\Downloads\\Vishrut Aryan\\tf-venv\\lib\\site-packages\\transformers\\modeling_tf_pytorch_utils.py:143\u001b[0m, in \u001b[0;36mapply_transpose\u001b[1;34m(transpose, weight, match_shape, pt_to_tf)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 143\u001b[0m     weight \u001b[39m=\u001b[39m reshape(weight, match_shape)\n\u001b[0;32m    144\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\cl502_20\\Downloads\\Vishrut Aryan\\tf-venv\\lib\\site-packages\\transformers\\utils\\generic.py:599\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(array, newshape)\u001b[0m\n\u001b[0;32m    597\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m--> 599\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mreshape(array, newshape)\n\u001b[0;32m    600\u001b[0m \u001b[39melif\u001b[39;00m is_jax_tensor(array):\n",
      "File \u001b[1;32mc:\\Users\\cl502_20\\Downloads\\Vishrut Aryan\\tf-venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\cl502_20\\Downloads\\Vishrut Aryan\\tf-venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input to reshape is a tensor with 5120 values, but the requested shape has 3072 [Op:Reshape]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\cl502_20\\Downloads\\Vishrut Aryan\\zeroshot_patents_bart.ipynb Cell 31\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/cl502_20/Downloads/Vishrut%20Aryan/zeroshot_patents_bart.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m loaded_model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfrom_pretrained(output_dir) \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cl502_20/Downloads/Vishrut%20Aryan/zeroshot_patents_bart.ipynb#X42sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m loaded_tokenizer \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mfrom_pretrained(output_dir)\n",
      "File \u001b[1;32mc:\\Users\\cl502_20\\Downloads\\Vishrut Aryan\\tf-venv\\lib\\site-packages\\transformers\\modeling_tf_utils.py:2921\u001b[0m, in \u001b[0;36mTFPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2915\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodeling_tf_pytorch_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m load_pytorch_state_dict_in_tf2_model\n\u001b[0;32m   2917\u001b[0m     \u001b[39mwith\u001b[39;00m safe_open(resolved_archive_file, framework\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m safetensors_archive:\n\u001b[0;32m   2918\u001b[0m         \u001b[39m# Load from a PyTorch checkpoint\u001b[39;00m\n\u001b[0;32m   2919\u001b[0m         \u001b[39m# We load in TF format here because PT weights often need to be transposed, and this is much\u001b[39;00m\n\u001b[0;32m   2920\u001b[0m         \u001b[39m# faster on GPU. Loading as numpy and transposing on CPU adds several seconds to load times.\u001b[39;00m\n\u001b[1;32m-> 2921\u001b[0m         \u001b[39mreturn\u001b[39;00m load_pytorch_state_dict_in_tf2_model(\n\u001b[0;32m   2922\u001b[0m             model,\n\u001b[0;32m   2923\u001b[0m             safetensors_archive,\n\u001b[0;32m   2924\u001b[0m             tf_inputs\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,  \u001b[39m# No need to build the model again\u001b[39;49;00m\n\u001b[0;32m   2925\u001b[0m             allow_missing_keys\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   2926\u001b[0m             output_loading_info\u001b[39m=\u001b[39;49moutput_loading_info,\n\u001b[0;32m   2927\u001b[0m             _prefix\u001b[39m=\u001b[39;49mload_weight_prefix,\n\u001b[0;32m   2928\u001b[0m             ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[0;32m   2929\u001b[0m             tf_to_pt_weight_rename\u001b[39m=\u001b[39;49mtf_to_pt_weight_rename,\n\u001b[0;32m   2930\u001b[0m         )\n\u001b[0;32m   2932\u001b[0m \u001b[39m# 'by_name' allow us to do transfer learning by skipping/adding layers\u001b[39;00m\n\u001b[0;32m   2933\u001b[0m \u001b[39m# see https://github.com/tensorflow/tensorflow/blob/00fad90125b18b80fe054de1055770cfb8fe4ba3/tensorflow/python/keras/engine/network.py#L1339-L1357\u001b[39;00m\n\u001b[0;32m   2934\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\cl502_20\\Downloads\\Vishrut Aryan\\tf-venv\\lib\\site-packages\\transformers\\modeling_tf_pytorch_utils.py:340\u001b[0m, in \u001b[0;36mload_pytorch_state_dict_in_tf2_model\u001b[1;34m(tf_model, pt_state_dict, tf_inputs, allow_missing_keys, output_loading_info, _prefix, tf_to_pt_weight_rename, ignore_mismatched_sizes)\u001b[0m\n\u001b[0;32m    336\u001b[0m     error_msg \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(e)\n\u001b[0;32m    337\u001b[0m     error_msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m    338\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    339\u001b[0m     )\n\u001b[1;32m--> 340\u001b[0m     \u001b[39mraise\u001b[39;00m tf\u001b[39m.\u001b[39;49merrors\u001b[39m.\u001b[39;49mInvalidArgumentError(error_msg)\n\u001b[0;32m    341\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    342\u001b[0m     mismatched_keys\u001b[39m.\u001b[39mappend((name, array\u001b[39m.\u001b[39mshape, symbolic_weight\u001b[39m.\u001b[39mshape))\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'op' and 'message'"
     ]
    }
   ],
   "source": [
    "loaded_model = model.from_pretrained(output_dir) # type: ignore\n",
    "loaded_tokenizer = tokenizer.from_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "premise = \"ai powered multi stage adjustment\"\n",
    "hypothesis = \"smart water filtration device\"\n",
    "input_ids = tokenizer(premise, hypothesis, truncation=True, padding=True, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loaded_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\cl502_20\\Downloads\\Vishrut Aryan\\zeroshot_patents_bart.ipynb Cell 33\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/cl502_20/Downloads/Vishrut%20Aryan/zeroshot_patents_bart.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m outputs \u001b[39m=\u001b[39m loaded_model(input_ids)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cl502_20/Downloads/Vishrut%20Aryan/zeroshot_patents_bart.ipynb#X44sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cl502_20/Downloads/Vishrut%20Aryan/zeroshot_patents_bart.ipynb#X44sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m probabilities \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39msoftmax(logits, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loaded_model' is not defined"
     ]
    }
   ],
   "source": [
    "outputs = loaded_model(input_ids)\n",
    "logits = outputs.logits\n",
    "\n",
    "probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "predicted_class = tf.argmax(probabilities, axis=-1)\n",
    "predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = tf.linspace(1.0, 3.0, num=3)\n",
    "expected_score = tf.reduce_sum(probabilities * scores, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score = 3.0\n",
    "normalized_score = expected_score / max_score\n",
    "rounded_score = tf.round(normalized_score * 4) / 4\n",
    "clamped_score = tf.minimum(rounded_score, tf.constant(1.00))\n",
    "\n",
    "formatted_output = clamped_score.numpy()\n",
    "formatted_output_str = [\"{:.2f}\".format(float(score)) for score in formatted_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9221207499504089\n",
      "Unrelated\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "score_to_label_mapping = {\n",
    "    0.00: \"Very close match\",\n",
    "    0.25: \"Close synonym\",\n",
    "    0.50: \"Synonyms which don’t have the same meaning (same function, same properties)\",\n",
    "    0.75: \"Somewhat related\",\n",
    "    1.00: \"Unrelated\"\n",
    "}\n",
    "\n",
    "# Make sure to convert the numpy array to a float\n",
    "rounded_score_value = float(rounded_score.numpy()[0])\n",
    "print(float(normalized_score.numpy()[0]))\n",
    "\n",
    "# You don't need to format it as a string, use the float value directly for lookup\n",
    "label = score_to_label_mapping.get(rounded_score_value, \"Label not found\")\n",
    "print(label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
