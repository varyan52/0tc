{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cl502_20\\Downloads\\Vishrut Aryan\\tf-venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 52.0/52.0 [00:00<00:00, 4.57kB/s]\n",
      "c:\\Users\\cl502_20\\Downloads\\Vishrut Aryan\\tf-venv\\lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\cl502_20\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 1.24MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 804kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 475/475 [00:00<00:00, 674kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 1.63G/1.63G [00:16<00:00, 102MB/s] \n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-large and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'classifier.weight', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from transformers import DebertaTokenizer, DebertaForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-large\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "model = DebertaForSequenceClassification.from_pretrained(\"microsoft/deberta-large\", num_labels=5)\n",
    "\n",
    "model.config.dropout = 0.5\n",
    "model.classifier = torch.nn.Linear(in_features=model.config.hidden_size, out_features=5, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_dataset = pd.DataFrame(pd.read_csv('train_patents.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mapping = {\n",
    "    0: 0,\n",
    "    0.25: 1,\n",
    "    0.5: 2,\n",
    "    0.75: 3,\n",
    "    1: 4\n",
    "}\n",
    "\n",
    "train_dataset['score'] = train_dataset['score'].map(score_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_dataset, cross_verify_data = train_test_split(train_dataset, test_size=0.1)  # 10% for cross-verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cl502_20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from random import randint\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def replace_synonym(sentence, num_replacements=1):\n",
    "    words = sentence.split()\n",
    "    new_sentence = sentence\n",
    "    for _ in range(num_replacements):\n",
    "        word_to_replace = words[randint(0, len(words)-1)]\n",
    "        synonyms = [syn.name().split('.')[0] for syn in wordnet.synsets(word_to_replace) if syn.name().split('.')[0] != word_to_replace]\n",
    "        if synonyms:\n",
    "            new_sentence = new_sentence.replace(word_to_replace, synonyms[0], 1)\n",
    "    return new_sentence\n",
    "\n",
    "# Augmenting data\n",
    "df_train_dataset['target'] = df_train_dataset['target'].apply(lambda x: replace_synonym(x, num_replacements=5))\n",
    "df_train_dataset['anchor'] = df_train_dataset['anchor'].apply(lambda x: replace_synonym(x, num_replacements=5))\n",
    "cross_verify_data['target'] = cross_verify_data['target'].apply(lambda x: replace_synonym(x, num_replacements=5))\n",
    "cross_verify_data['anchor'] = cross_verify_data['anchor'].apply(lambda x: replace_synonym(x, num_replacements=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score\n",
       "2        17\n",
       "1        16\n",
       "0        11\n",
       "3         4\n",
       "4         2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_actual = df_train_dataset['score']\n",
    "score_list = score_actual[1:1001].tolist()\n",
    "score_train = pd.DataFrame(score_actual[1:1001])\n",
    "score_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acme position',\n",
       " 'prolog',\n",
       " 'encapsulate paint',\n",
       " 'axile propagation',\n",
       " 'carburization',\n",
       " 'opc barrel',\n",
       " 'wearability',\n",
       " 'planar gearinginging set',\n",
       " 'conductor atom',\n",
       " 'acerb assimilation',\n",
       " 'selectively predetermine',\n",
       " 'wall military_military_military_post',\n",
       " 'tax_tax_tax_return social_organization',\n",
       " 'transport aside chopine',\n",
       " 'push fall',\n",
       " 'radio_receiver wave transmittance',\n",
       " 'electric newcomer',\n",
       " 'pneumatic logic',\n",
       " 'relational recipe',\n",
       " 'traffic_circle electric',\n",
       " 'hinge mechanism',\n",
       " 'angular liaison carriage',\n",
       " 'run control_condition_condition valve',\n",
       " 'ammonia_water convalescence',\n",
       " 'combine with ocular component',\n",
       " 'good heart',\n",
       " 'unlike circumferential position',\n",
       " 'goal of parallel_parallel_parallel_bars',\n",
       " 'angular contact carriage',\n",
       " 'boom hydraulic cylinder',\n",
       " 'rotatable bedroom',\n",
       " 'good heart',\n",
       " 'determine substrate',\n",
       " 'determine substrate',\n",
       " 'propyl platitude',\n",
       " 'fabric constitution',\n",
       " 'chief pulsation laser',\n",
       " 'important_person',\n",
       " 'succession conservation',\n",
       " 'hydrocarbyl substitute succinic',\n",
       " 'reflection character liquid quartz_glass expose',\n",
       " 'beginning groove',\n",
       " 'inbound change_of_location',\n",
       " 'display isotropy',\n",
       " 'mayenite',\n",
       " 'do work operation',\n",
       " 'topographic_point to stage_set',\n",
       " 'oppose wall',\n",
       " 'stream opening',\n",
       " 'push fall']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_dataset['hypothesis'] = df_train_dataset['anchor']\n",
    "inputs = df_train_dataset['hypothesis']\n",
    "hypothesis_list = inputs[1:1001].tolist()\n",
    "hypothesis_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acme plan acme',\n",
       " 'warhead',\n",
       " 'encapsulate component',\n",
       " 'propagation corner',\n",
       " 'hotness center infection',\n",
       " 'actuator',\n",
       " 'absorbent_material_material_material polymer',\n",
       " 'coplanar stage_stage_set',\n",
       " 'metallic_element decoration',\n",
       " 'acidic submergence',\n",
       " 'selective bias',\n",
       " 'radio',\n",
       " 'tax_tax_tax_return argument',\n",
       " 'transport aside rake circulation',\n",
       " 'push fall mechanism',\n",
       " 'watery radio_receiver_receiver wave',\n",
       " 'electric newcomer drive',\n",
       " 'pneumatic control_condition_condition_condition device',\n",
       " 'environment component data',\n",
       " 'dynamo electric',\n",
       " 'hinge mechanism',\n",
       " 'curler carriage',\n",
       " 'hydraulic aerodynamic_lift',\n",
       " 'recovery from wound',\n",
       " 'merge with ocular component',\n",
       " 'syringe',\n",
       " 'lapp intervals',\n",
       " 'dwell of parallel_parallel_parallel_bars',\n",
       " 'shape angular liaison bearing',\n",
       " 'air press instrument',\n",
       " 'chattel enclosure',\n",
       " 'fracture with telescoping extremity',\n",
       " 'layer',\n",
       " 'phonograph_record determine substrate',\n",
       " 'methyl iodide',\n",
       " 'process',\n",
       " 'chief laser',\n",
       " 'significant fictional_fictional_fictional_fictional_character',\n",
       " 'affinity',\n",
       " 'acerb acerb',\n",
       " 'reflective mode liquid_crystal_display expose',\n",
       " 'visible light lamp',\n",
       " 'time change_of_location',\n",
       " 'display axile isotropy',\n",
       " 'high frequency sound',\n",
       " 'arrest work operation',\n",
       " 'function expose',\n",
       " 'opposed slope wall',\n",
       " 'consumption opening formed',\n",
       " 'swoop extremity']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "premise = df_train_dataset['target']\n",
    "premise_list = premise[1:1001].tolist()\n",
    "premise_for_testing = pd.DataFrame(premise_list)\n",
    "premise_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "input_ids = tokenizer(premise_list, hypothesis_list, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "attention_masks = input_ids[\"attention_mask\"]\n",
    "labels = torch.tensor(score_list, dtype=torch.long)\n",
    "labels_one_hot = torch.nn.functional.one_hot(labels, num_classes=5)\n",
    "labels_one_hot = labels_one_hot.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Up Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encoded_texts, labels):\n",
    "        self.encoded_texts = encoded_texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encoded_texts.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "train_dataset = CustomDataset(input_ids, labels_one_hot)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:07<00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 7.5497, 'train_samples_per_second': 6.623, 'train_steps_per_second': 1.325, 'train_loss': 10.793844604492188, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=10.793844604492188, metrics={'train_runtime': 7.5497, 'train_samples_per_second': 6.623, 'train_steps_per_second': 1.325, 'train_loss': 10.793844604492188, 'epoch': 1.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./patents-output\",\n",
    "    per_device_train_batch_size = 5,\n",
    "    num_train_epochs = 1,\n",
    "    learning_rate = 1e-2,\n",
    "    save_steps = 500,\n",
    "    save_total_limit = 2,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps = 500,\n",
    "    max_grad_norm=1.0,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model, # type: ignore\n",
    "    args = training_args,\n",
    "    data_collator = data_collator,\n",
    "    train_dataset = train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_verify_hypothesis = cross_verify_data['anchor'].tolist()\n",
    "cross_verify_premise = cross_verify_data['target'].tolist()\n",
    "cross_verify_score = cross_verify_data['score'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_verify_input_ids = tokenizer(cross_verify_premise, cross_verify_hypothesis, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "cross_verify_score_tensor = torch.tensor(cross_verify_score, dtype=torch.long)\n",
    "cross_verify_labels_one_hot = torch.nn.functional.one_hot(cross_verify_score_tensor, num_classes=5).float()\n",
    "cross_verify_dataset = CustomDataset(cross_verify_input_ids, cross_verify_labels_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 456/456 [00:40<00:00, 11.25it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "cross_verify_results = trainer.predict(cross_verify_dataset)\n",
    "cross_verify_predictions = cross_verify_results.predictions\n",
    "cross_verify_label_ids = cross_verify_results.label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       767\n",
      "           1       1.00      1.00      1.00      1152\n",
      "           2       1.00      1.00      1.00      1191\n",
      "           3       1.00      1.00      1.00       406\n",
      "           4       1.00      1.00      1.00       132\n",
      "\n",
      "   micro avg       1.00      1.00      1.00      3648\n",
      "   macro avg       1.00      1.00      1.00      3648\n",
      "weighted avg       1.00      1.00      1.00      3648\n",
      " samples avg       1.00      1.00      1.00      3648\n",
      "\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(classification_report(cross_verify_labels_one_hot, cross_verify_label_ids)) # type: ignore\n",
    "print(\"Accuracy:\", accuracy_score(cross_verify_labels_one_hot, cross_verify_label_ids)) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: id\n",
      "True Label: tensor([0., 0., 1., 0., 0.])\n",
      "Predicted Label: [0. 0. 1. 0. 0.]\n",
      "--------------------------------------------------\n",
      "Input: anchor\n",
      "True Label: tensor([0., 0., 1., 0., 0.])\n",
      "Predicted Label: [0. 0. 1. 0. 0.]\n",
      "--------------------------------------------------\n",
      "Input: target\n",
      "True Label: tensor([0., 1., 0., 0., 0.])\n",
      "Predicted Label: [0. 1. 0. 0. 0.]\n",
      "--------------------------------------------------\n",
      "Input: context\n",
      "True Label: tensor([0., 0., 1., 0., 0.])\n",
      "Predicted Label: [0. 0. 1. 0. 0.]\n",
      "--------------------------------------------------\n",
      "Input: score\n",
      "True Label: tensor([0., 0., 1., 0., 0.])\n",
      "Predicted Label: [0. 0. 1. 0. 0.]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for input, true_label, predicted_label in zip(cross_verify_data[:10], cross_verify_labels_one_hot[:10], cross_verify_label_ids[:10]): # type: ignore\n",
    "    print(f\"Input: {input}\")\n",
    "    print(f\"True Label: {true_label}\")\n",
    "    print(f\"Predicted Label: {predicted_label}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./patents-output/deberta\\\\tokenizer_config.json',\n",
       " './patents-output/deberta\\\\special_tokens_map.json',\n",
       " './patents-output/deberta\\\\vocab.json',\n",
       " './patents-output/deberta\\\\merges.txt',\n",
       " './patents-output/deberta\\\\added_tokens.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = \"./patents-output/deberta\"\n",
    "\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '\\x18'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\cl502_20\\Downloads\\Vishrut Aryan\\zeroshot_patents_deberta.ipynb Cell 23\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cl502_20/Downloads/Vishrut%20Aryan/zeroshot_patents_deberta.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoConfig\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cl502_20/Downloads/Vishrut%20Aryan/zeroshot_patents_deberta.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m config \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39mfrom_pretrained(output_dir)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/cl502_20/Downloads/Vishrut%20Aryan/zeroshot_patents_deberta.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m tf_model \u001b[39m=\u001b[39m TFAutoModelForSequenceClassification\u001b[39m.\u001b[39;49mfrom_pretrained(output_dir, from_pt\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, config\u001b[39m=\u001b[39;49mconfig)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cl502_20/Downloads/Vishrut%20Aryan/zeroshot_patents_deberta.ipynb#X31sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m tf_model\u001b[39m.\u001b[39msave_pretrained(output_dir, saved_model\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cl502_20/Downloads/Vishrut%20Aryan/zeroshot_patents_deberta.ipynb#X31sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m tf_model\u001b[39m.\u001b[39msave_weights(output_dir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/tf_model.h5\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\cl502_20\\Downloads\\Vishrut Aryan\\tf-venv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    565\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 566\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    567\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39mmodel_args, config\u001b[39m=\u001b[39mconfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    568\u001b[0m     )\n\u001b[0;32m    569\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    570\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    571\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    572\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\cl502_20\\Downloads\\Vishrut Aryan\\tf-venv\\lib\\site-packages\\transformers\\modeling_tf_utils.py:2898\u001b[0m, in \u001b[0;36mTFPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2895\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodeling_tf_pytorch_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m load_pytorch_checkpoint_in_tf2_model\n\u001b[0;32m   2897\u001b[0m     \u001b[39m# Load from a PyTorch checkpoint\u001b[39;00m\n\u001b[1;32m-> 2898\u001b[0m     \u001b[39mreturn\u001b[39;00m load_pytorch_checkpoint_in_tf2_model(\n\u001b[0;32m   2899\u001b[0m         model,\n\u001b[0;32m   2900\u001b[0m         resolved_archive_file,\n\u001b[0;32m   2901\u001b[0m         allow_missing_keys\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   2902\u001b[0m         output_loading_info\u001b[39m=\u001b[39;49moutput_loading_info,\n\u001b[0;32m   2903\u001b[0m         _prefix\u001b[39m=\u001b[39;49mload_weight_prefix,\n\u001b[0;32m   2904\u001b[0m         tf_to_pt_weight_rename\u001b[39m=\u001b[39;49mtf_to_pt_weight_rename,\n\u001b[0;32m   2905\u001b[0m     )\n\u001b[0;32m   2907\u001b[0m \u001b[39m# we might need to extend the variable scope for composite models\u001b[39;00m\n\u001b[0;32m   2908\u001b[0m \u001b[39mif\u001b[39;00m load_weight_prefix \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\cl502_20\\Downloads\\Vishrut Aryan\\tf-venv\\lib\\site-packages\\transformers\\modeling_tf_pytorch_utils.py:185\u001b[0m, in \u001b[0;36mload_pytorch_checkpoint_in_tf2_model\u001b[1;34m(tf_model, pytorch_checkpoint_path, tf_inputs, allow_missing_keys, output_loading_info, _prefix, tf_to_pt_weight_rename)\u001b[0m\n\u001b[0;32m    183\u001b[0m     pt_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(path)\n\u001b[0;32m    184\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoading PyTorch weights from \u001b[39m\u001b[39m{\u001b[39;00mpt_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 185\u001b[0m     pt_state_dict\u001b[39m.\u001b[39mupdate(torch\u001b[39m.\u001b[39;49mload(pt_path, map_location\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m    187\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPyTorch checkpoint contains \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39msum\u001b[39m(t\u001b[39m.\u001b[39mnumel()\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mt\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39mpt_state_dict\u001b[39m.\u001b[39mvalues())\u001b[39m:\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m parameters\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    189\u001b[0m \u001b[39mreturn\u001b[39;00m load_pytorch_weights_in_tf2_model(\n\u001b[0;32m    190\u001b[0m     tf_model,\n\u001b[0;32m    191\u001b[0m     pt_state_dict,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    196\u001b[0m     tf_to_pt_weight_rename\u001b[39m=\u001b[39mtf_to_pt_weight_rename,\n\u001b[0;32m    197\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\cl502_20\\Downloads\\Vishrut Aryan\\tf-venv\\lib\\site-packages\\torch\\serialization.py:1028\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1026\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1027\u001b[0m         \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1028\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n",
      "File \u001b[1;32mc:\\Users\\cl502_20\\Downloads\\Vishrut Aryan\\tf-venv\\lib\\site-packages\\torch\\serialization.py:1246\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1240\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(f, \u001b[39m'\u001b[39m\u001b[39mreadinto\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m (\u001b[39m3\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mversion_info \u001b[39m<\u001b[39m (\u001b[39m3\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m2\u001b[39m):\n\u001b[0;32m   1241\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1242\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtorch.load does not work with file-like objects that do not implement readinto on Python 3.8.0 and 3.8.1. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1243\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReceived object of type \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(f)\u001b[39m}\u001b[39;00m\u001b[39m\\\"\u001b[39;00m\u001b[39m. Please update to Python 3.8.2 or newer to restore this \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1244\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfunctionality.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1246\u001b[0m magic_number \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mload(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1247\u001b[0m \u001b[39mif\u001b[39;00m magic_number \u001b[39m!=\u001b[39m MAGIC_NUMBER:\n\u001b[0;32m   1248\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid magic number; corrupt file?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: invalid load key, '\\x18'."
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(output_dir)\n",
    "tf_model = TFAutoModelForSequenceClassification.from_pretrained(output_dir, from_pt=True, config=config)\n",
    "tf_model.save_pretrained(output_dir, saved_model=True)\n",
    "tf_model.save_weights(output_dir + '/tf_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_model = model.from_pretrained('./patents-output/bart')\n",
    "#loaded_tokenizer = tokenizer.from_pretrained('./patents-output/bart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current CUDA device index: 0\n",
      "Current CUDA device name: NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the current CUDA device index\n",
    "    current_device = torch.cuda.current_device()\n",
    "    print(f\"Current CUDA device index: {current_device}\")\n",
    "\n",
    "    # Get the name of the current CUDA device\n",
    "    current_device_name = torch.cuda.get_device_name(current_device)\n",
    "    print(f\"Current CUDA device name: {current_device_name}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBartForSequenceClassification: ['model.decoder.version', 'model.encoder.version']\n",
      "- This IS expected if you are initializing TFBartForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBartForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBartForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBartForSequenceClassification, BartTokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "# Assign the model and tokenizer\n",
    "model = TFBartForSequenceClassification.from_pretrained(\"facebook/bart-large-mnli\")\n",
    "model.config.dropout = 0.5\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model.classification_head.dense = Dense(model.config.d_model, activation='linear', use_bias=True)\n",
    "model.classification_head.out_proj = Dense(5, activation='linear', use_bias=True)\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-mnli\",  return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at C:/Users/aryan/Actual-Coding/CDAC/patents-output/bart were not used when initializing TFBartForSequenceClassification: ['final_logits_bias']\n",
      "- This IS expected if you are initializing TFBartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBartForSequenceClassification were not initialized from the model checkpoint at C:/Users/aryan/Actual-Coding/CDAC/patents-output/bart and are newly initialized: ['classification_head']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "loaded_model = model.from_pretrained('C:/Users/aryan/Actual-Coding/CDAC/patents-output/bart') # type: ignore\n",
    "loaded_tokenizer = tokenizer.from_pretrained('C:/Users/aryan/Actual-Coding/CDAC/patents-output/bart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "premise = \"ai powered multi stage adjustment\"\n",
    "hypothesis = \"smart water filtration device\"\n",
    "input_ids = tokenizer(premise, hypothesis, truncation=True, padding=True, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=int64, numpy=array([2], dtype=int64)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = loaded_model(input_ids)\n",
    "logits = outputs.logits\n",
    "\n",
    "probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "predicted_class = tf.argmax(probabilities, axis=-1)\n",
    "predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = tf.linspace(1.0, 3.0, num=3)\n",
    "expected_score = tf.reduce_sum(probabilities * scores, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score = 3.0\n",
    "normalized_score = expected_score / max_score\n",
    "rounded_score = tf.round(normalized_score * 4) / 4\n",
    "clamped_score = tf.minimum(rounded_score, tf.constant(1.00))\n",
    "\n",
    "formatted_output = clamped_score.numpy()\n",
    "formatted_output_str = [\"{:.2f}\".format(float(score)) for score in formatted_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7048661708831787\n",
      "Somewhat related\n"
     ]
    }
   ],
   "source": [
    "score_to_label_mapping = {\n",
    "    0.00: \"Very close match\",\n",
    "    0.25: \"Close synonym\",\n",
    "    0.50: \"Synonyms which don’t have the same meaning (same function, same properties)\",\n",
    "    0.75: \"Somewhat related\",\n",
    "    1.00: \"Unrelated\"\n",
    "}\n",
    "\n",
    "# Make sure to convert the numpy array to a float\n",
    "rounded_score_value = float(rounded_score.numpy()[0])\n",
    "print(float(normalized_score.numpy()[0]))\n",
    "\n",
    "# You don't need to format it as a string, use the float value directly for lookup\n",
    "label = score_to_label_mapping.get(rounded_score_value, \"Label not found\")\n",
    "print(label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
